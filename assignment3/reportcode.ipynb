{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22bdbe71",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 20px; font-weight: 600;text-transform:uppercase; color:#23547b; background:#B3E5FC;text-align:center\">\n",
    "<h1 > BBM 409: Introduction to Machine Learning Lab. </h1>\n",
    "<h1> Assignment - 3 </h1>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\"> \n",
    "<h2 > Numan Kafadar 21946242</h2>\n",
    "<h2> Umut Güngör 21946198</h2>\n",
    "</div>\n",
    "<h1  style=\"color:#805b36 ;background:#FFD8B2\"> Importing Necessary Libraries and Reading Dataset</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0765cf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ec954c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>312</td>\n",
       "      <td>lacroix label bought by us firm luxury goods g...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>26</td>\n",
       "      <td>moya fights back for indian title carlos moya ...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>1387</td>\n",
       "      <td>adriano s chelsea link rejected adriano s agen...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>1651</td>\n",
       "      <td>brown s poll campaign move denied the governme...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>30</td>\n",
       "      <td>halloween writer debra hill dies screenwriter ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>1376</td>\n",
       "      <td>liberian economy starts to grow the liberian e...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>1984</td>\n",
       "      <td>officials respond in court row australian tenn...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>1081</td>\n",
       "      <td>outkast win at mtv europe awards us hip-hop du...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>1925</td>\n",
       "      <td>straw to attend auschwitz service foreign secr...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1380</th>\n",
       "      <td>208</td>\n",
       "      <td>blair to face trust issue head on tony blair s...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ArticleId                                               Text  \\\n",
       "1341        312  lacroix label bought by us firm luxury goods g...   \n",
       "387          26  moya fights back for indian title carlos moya ...   \n",
       "253        1387  adriano s chelsea link rejected adriano s agen...   \n",
       "1329       1651  brown s poll campaign move denied the governme...   \n",
       "950          30  halloween writer debra hill dies screenwriter ...   \n",
       "245        1376  liberian economy starts to grow the liberian e...   \n",
       "649        1984  officials respond in court row australian tenn...   \n",
       "1011       1081  outkast win at mtv europe awards us hip-hop du...   \n",
       "1081       1925  straw to attend auschwitz service foreign secr...   \n",
       "1380        208  blair to face trust issue head on tony blair s...   \n",
       "\n",
       "           Category  \n",
       "1341       business  \n",
       "387           sport  \n",
       "253           sport  \n",
       "1329       politics  \n",
       "950   entertainment  \n",
       "245        business  \n",
       "649           sport  \n",
       "1011  entertainment  \n",
       "1081       politics  \n",
       "1380       politics  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exploring the dataset\n",
    "df = pd.read_csv(\"English_Dataset.csv\")\n",
    "\n",
    "#shuffle the dataset\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9d62af3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sport            346\n",
      "business         336\n",
      "politics         274\n",
      "entertainment    273\n",
      "tech             261\n",
      "Name: Category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#all the categories and their counts on dataset\n",
    "print(df[\"Category\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357399dc",
   "metadata": {},
   "source": [
    "If we look at the counts of each category, we can say that our dataset is quite balanced. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecd3968",
   "metadata": {},
   "source": [
    "<h1  style=\"color:#805b36 ;background:#FFD8B2\"> Splitting Data into Train and Test Samples , Defining Necessary Global Variables</h1>\n",
    "\n",
    "Empirical studies show that the best results are obtained if we use 20-30% of the data for testing, and the remaining 70-80% of the data for training. So we will use %70 of our data as training set and remaining %30 of our data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "463eaaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train and test samples\n",
    "X = df[\"Text\"].to_numpy()\n",
    "y = df[\"Category\"].to_numpy()\n",
    "\n",
    "#%70 training, %30 testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "#label_values\n",
    "label_values = df[\"Category\"].unique()\n",
    "#ngram parameters\n",
    "ngrams = { \"unigram\":(1,1), \"bigram\": (2,2)} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a373fe41",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px; font-weight: 600;text-transform:uppercase; color:#256029; background:#C8E6C9\"> Part 1:  Understanding the data </h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e4613242",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get occurences words list of each category\n",
    "def most_occurence_values(df, labels, unmeaningful_words=set()):\n",
    "    occurence_dict={}\n",
    "    #for each category\n",
    "    for l in labels:\n",
    "        data = df[df[\"Category\"]==l]\n",
    "        word_counts = {}\n",
    "        total_count = 0\n",
    "        #for each text in category \n",
    "        for text in data[\"Text\"].to_numpy():\n",
    "            word_list = text.split()\n",
    "            #for each word in text\n",
    "            for word in word_list:\n",
    "                #increase the count of word \n",
    "                if word in unmeaningful_words:\n",
    "                    continue\n",
    "                elif word in word_counts:\n",
    "                    word_counts[word] += 1\n",
    "                    total_count += 1\n",
    "                else:\n",
    "                    word_counts[word] = 1\n",
    "                    total_count += 1\n",
    "        #sort words list according to their occurence count\n",
    "        word_counts = dict(sorted(word_counts.items(), key=lambda item: item[1], reverse = True))\n",
    "        #set occurence counts to occurence frequency\n",
    "        for word in word_counts:\n",
    "            word_counts[word] = word_counts[word]/total_count\n",
    "        \n",
    "        occurence_dict[l] = word_counts\n",
    "    return occurence_dict\n",
    "\n",
    "#print first n words that has highest occurence frequency\n",
    "def get_n_occurence_values(df,labels, n=3, unmeaningful_words=set()):\n",
    "    #occurence dictionary\n",
    "    occurences = most_occurence_values(df,label_values, unmeaningful_words)\n",
    "    #for each category\n",
    "    for l in occurences:\n",
    "        #words and their frequencies as array\n",
    "        word_list = list(occurences[l].keys())\n",
    "        word_list_freqs = list(occurences[l].values())\n",
    "        print(\"For {} label , most occurence {} words are :\".format(l,n))\n",
    "        for i in range(n):\n",
    "            print(\"{:<12}  {:>12}\".format(word_list[i],(\"frequency : %\" +str(word_list_freqs[i]*100))))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "26eb773b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For business label , most occurence 3 words are :\n",
      "the           frequency : %6.349248759807982\n",
      "to            frequency : %2.932820334695986\n",
      "of            frequency : %2.548071356685459\n",
      "\n",
      "For sport label , most occurence 3 words are :\n",
      "the           frequency : %5.69680255106438\n",
      "to            frequency : %2.7346375937257608\n",
      "a             frequency : %2.2813065586486254\n",
      "\n",
      "For politics label , most occurence 3 words are :\n",
      "the           frequency : %6.452136509353569\n",
      "to            frequency : %3.1700685793125833\n",
      "of            frequency : %2.3024794059164875\n",
      "\n",
      "For entertainment label , most occurence 3 words are :\n",
      "the           frequency : %6.375743215077119\n",
      "and           frequency : %2.325632418438316\n",
      "to            frequency : %2.305886482810066\n",
      "\n",
      "For tech label , most occurence 3 words are :\n",
      "the           frequency : %5.718975455204794\n",
      "to            frequency : %3.125548726953468\n",
      "of            frequency : %2.603351528801008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_n_occurence_values(df,label_values, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e4cc01",
   "metadata": {},
   "source": [
    "As we can see above, words that has highest occurence frequency are a bit useless for analyzing data. Because these words are the common words like prepositions,conjunctions and pronouns in English language that has no effects on specifying category of the text. Alternatively, we can discard these words and do the same process again like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c0e01a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For business label , most occurence 3 words are :\n",
      "year          frequency : %0.4980621753112888\n",
      "firm          frequency : %0.3991094252494434\n",
      "company       frequency : %0.39581100024738186\n",
      "\n",
      "For sport label , most occurence 3 words are :\n",
      "england       frequency : %0.533546979408155\n",
      "game          frequency : %0.48581753716078\n",
      "win           frequency : %0.4449065866630302\n",
      "\n",
      "For politics label , most occurence 3 words are :\n",
      "labour        frequency : %0.7601050209069398\n",
      "government    frequency : %0.6968979935820556\n",
      "blair         frequency : %0.6028977990988946\n",
      "\n",
      "For entertainment label , most occurence 3 words are :\n",
      "film          frequency : %1.0405313701700631\n",
      "best          frequency : %0.8307799872504061\n",
      "music         frequency : %0.47708157683686686\n",
      "\n",
      "For tech label , most occurence 3 words are :\n",
      "people        frequency : %0.9334190961990098\n",
      "mobile        frequency : %0.4652136841632885\n",
      "technology    frequency : %0.3934122152249031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#words that has no meanings at all\n",
    "my_set = (\"-\", \"s\", \"t\", \"m\", \"o\", \"mr\",\"said\", \"just\",\"new\")       # adding these words into stop words\n",
    "unmeaningful_words = set(ENGLISH_STOP_WORDS).union(my_set)\n",
    "\n",
    "#occurences without unnecessary words\n",
    "get_n_occurence_values(df,label_values, 3,unmeaningful_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc33952",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#694382; background: #ECCFFF\"> Analysis Of Part 1 </h1>\n",
    "As we can see, these words are much more useful for specifying the category of text samples since these words are much more meaningful for us. For example, in entertainment category, the meaningful words that appear most are \"film\", \"best\" and \"music\". It is feasible since that these words are relevant to the entertainment category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ded0ad",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px; font-weight: 600;text-transform:uppercase; color:#256029; background:#C8E6C9\"> Part 2:  Implementing Naive Bayes </h1>\n",
    "\n",
    "<h3 style=\"font-weight: 600;text-transform:uppercase; color:#c63737 ; background:#FFCDD2\"> Problem Definition,Solution Steps and Implementation</h3>\n",
    "<ul>\n",
    "        <li><span style=\"color:#c63737 ;\">Problem Definition </span>We will implement naive bayes model for classifying the categories of text samples. We will use Bag of Words (BoW) model which learns a vocabulary from all of the documents, then models each document by counting the number of times each word appears.\n",
    "  \n",
    "</li>  \n",
    "    <li><span style=\"color:#c63737 ; \">Solution Steps and Implementation : </span> \n",
    "        Bayes theorem for our case is as follows:\n",
    "        <img src=\"https://i.ibb.co/Y31Pzdc/bayes-Theoremforass.jpg\" alt=\"bayes-Theoremforass\" border=\"0\">\n",
    "        In the formula above, we will ignore P(text) part since it is appearing in every category. \n",
    "    Here our categories are sport, business, politics, entertainment and tech.<br> We will calculate each categories probabilities and predict according to their probabilities' closeseness of zero. Naive Bayes assumes that every word in the sentence is independent of the other words. So, we can look individual words probabilities in text like below:<br>\n",
    "    P( This is sport text |  category) = P(This |  category)P(is |  category)P(sport |  category)P(text |  category). <br><br>\n",
    "    However, sometimes word in the test data may not appear in train data. In this case, probability would be zero. To overcome this issue, we will use laplace smoothing. We will add 1 to dominator and length of words that appeared in training data to the denominator. In this case, our probability will never be 0 so that we won't have any 0 probaility values for each category.<br><br>\n",
    "     While classifying category, we are multiplying several probabilities together. This may cause to underflow since we are multiplying several small probabilities together. To solve this, we can take the logarithm of these probabilities. Since logarithms of the probabilities will have negative values, lower probabilities are further away from zero and larger probabilities are closer to zero. So we can still get the maximum of them. Also, since we take the logarithm of probabilities, we need to add them up instead of multiplying.\n",
    "    </li> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c8955a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#method for obtaining train and text word matrixes and also vocabulary list\n",
    "def get_CountVectorizer_results(x_train, x_test, ngram_range, stopwords=frozenset()):\n",
    "    #initilazing vectorizer\n",
    "    vect = CountVectorizer(ngram_range=ngram_range,stop_words=stopwords)\n",
    "    \n",
    "    #train and text vectors\n",
    "    train_cv = vect.fit_transform(x_train)\n",
    "    vocabulary_list = vect.get_feature_names_out()\n",
    "    test_cv = vect.transform(x_test)\n",
    "    \n",
    "    #train and test matrixes\n",
    "    train_words_matrix = train_cv.toarray()\n",
    "    test_words_matrix = test_cv.toarray()\n",
    "    \n",
    "    #return values\n",
    "    return train_words_matrix, test_words_matrix, vocabulary_list\n",
    "\n",
    "#fitting train data for later predicting phase\n",
    "def naive_bayes_fit(x_train, y_train, train_words_matrix, vocabulary_list, labels):\n",
    "    wordCountsOfLabels = {}  #each word counts in each category\n",
    "    total_counts_of_labels = {}  #total numbers of each category \n",
    "    #each categorys probabilities as log value\n",
    "    logs_of_each_labels = {}    #basically P(category)\n",
    "    \n",
    "    for l in labels:\n",
    "        #defaultdict used for hadling unseen word in test matrix \n",
    "        wordCountsOfLabels[l] = defaultdict(lambda : 0)\n",
    "        total_counts_of_labels[l] = len(x_train[np.where(y_train==l)])\n",
    "        logs_of_each_labels[l] = math.log(total_counts_of_labels[l] / len(y_train)) #P(category)=category/total\n",
    "    \n",
    "    #set word counts in each category for wordCountsOfLabels\n",
    "    #for each sample in training \n",
    "    for i in range(len(y_train)):\n",
    "        l=y_train[i] #category of text\n",
    "        #words that appeared in train sample\n",
    "        avaliable_words = np.array(np.where(train_words_matrix[i]!=0))[0]\n",
    "        for j in avaliable_words:\n",
    "            wordCountsOfLabels[l][vocabulary_list[j]] += train_words_matrix[i][j]\n",
    "    \n",
    "    #return values    \n",
    "    return wordCountsOfLabels, total_counts_of_labels, logs_of_each_labels\n",
    "\n",
    "def naive_bayes_predict(test_words_matrix, wordCountsOfLabels, total_counts_of_labels, logs_of_each_labels,\n",
    "                        labels , vocabulary_list):\n",
    "    \n",
    "    preds = [] #predictions\n",
    "    #for each test sample\n",
    "    for i in range(test_words_matrix.shape[0]):\n",
    "        #each categories probabilty values\n",
    "        label_scores = {}\n",
    "        #initialize each categories probabilty values with P(category)\n",
    "        for l in logs_of_each_labels:\n",
    "            label_scores[l] = logs_of_each_labels[l]\n",
    "        \n",
    "        #words in test sample\n",
    "        words_arr = []\n",
    "        for j in np.where(test_words_matrix[i]!=0)[0]:\n",
    "            words_arr.append(vocabulary_list[j])\n",
    "        \n",
    "        #for each word in test sample\n",
    "        for word in words_arr:\n",
    "            #for each category\n",
    "            for l in labels:\n",
    "                #laplace smoothing used for 0 probability , P(text|category)\n",
    "                #dominator = count of word in this category\n",
    "                #denominator=total count of category\n",
    "                dominator = wordCountsOfLabels[l][word] + 1 #1 is laplace smoothing \n",
    "                denominator = total_counts_of_labels[l] + len(vocabulary_list) #len(vocabulary_list) is laplace smoothing\n",
    "                #add to the label scores, we are adding because we use log probability\n",
    "                label_scores[l] += math.log(dominator/denominator)\n",
    "        #get category that has probability value closes to 0\n",
    "        preds.append(max(label_scores, key=label_scores.get))\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d6695288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Prediction Time : 1.2262709140777588 sn\n"
     ]
    }
   ],
   "source": [
    "#training and predicting-unigram- Stopwords not  used- TD-IDF not used\n",
    "st = time.time()\n",
    "train_words_matrix, test_words_matrix, vocabulary_list = get_CountVectorizer_results(X_train, X_test, ngrams[\"unigram\"])\n",
    "\n",
    "wordCountsOfLabels, total_counts_of_labels, logs_of_each_labels = naive_bayes_fit(X_train, y_train,\n",
    "                                                                                train_words_matrix, vocabulary_list,\n",
    "                                                                               label_values)\n",
    "\n",
    "y_preds_unigram_part2 = naive_bayes_predict(test_words_matrix, wordCountsOfLabels, total_counts_of_labels, logs_of_each_labels,\n",
    "                        label_values , vocabulary_list)\n",
    "\n",
    "print(\"Training and Prediction Time : {} sn\".format((time.time()-st)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fe395e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Prediction Time : 2.569582223892212 sn\n"
     ]
    }
   ],
   "source": [
    "#training and predicting-bigram- Stopwords not  used- TD-IDF not used\n",
    "st = time.time()\n",
    "train_words_matrix, test_words_matrix, vocabulary_list = get_CountVectorizer_results(X_train, X_test, ngrams[\"bigram\"])\n",
    "\n",
    "wordCountsOfLabels, total_counts_of_labels, logs_of_each_labels = naive_bayes_fit(X_train, y_train,\n",
    "                                                                                train_words_matrix, vocabulary_list,\n",
    "                                                                                label_values)\n",
    "\n",
    "y_preds_bigram_part2=naive_bayes_predict(test_words_matrix, wordCountsOfLabels, total_counts_of_labels, logs_of_each_labels,\n",
    "                        label_values , vocabulary_list)\n",
    "\n",
    "print(\"Training and Prediction Time : {} sn\".format((time.time()-st)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75090490",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#694382; background: #ECCFFF\"> Analysis Of Part 2 </h1>\n",
    "<ul>\n",
    "    <li>Since we used Bag of Words (BoW) model, our train and text matrix dimensions are quite large. Also when training bigram model, since we use every pair of words, our matrix is much large compared to unigram. This causes that our model training quite good in unigram and a bit slow in bigram.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998d9e4c",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px; font-weight: 600;text-transform:uppercase; color:#256029; background:#C8E6C9\"> Part 3:  Analyzing effect of the words on prediction and Stopwords </h1>\n",
    "<h1 style=\" color:#c63737 ; background:#FFCDD2\"> Part 3-a: Analyzing effect of the words on prediction </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c27f64d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_N_TfidfTransformer_values(df,category,n=10,ngram_range=(1,1), stopwords=frozenset()):\n",
    "    #get the category dataset\n",
    "    data = df[df[\"Category\"]==category]\n",
    "    data = data[\"Text\"]\n",
    "    #count vectorizer\n",
    "    vect = CountVectorizer(ngram_range=ngram_range, stop_words=stopwords)\n",
    "    data_cv = vect.fit_transform(data)\n",
    "    vocabulary = vect.get_feature_names_out()\n",
    "    #TfidfTransformer\n",
    "    tdif_Transformer = TfidfTransformer()\n",
    "    #passsing countvectorizer data array to TfidfTransformer\n",
    "    tdif_Transformer.fit_transform(data_cv.toarray())\n",
    "    #sort idf_ array\n",
    "    sorted_tdif = np.argsort(tdif_Transformer.idf_ )\n",
    "    #return last 10 elements of idf_ as presence and first 10 elements as absence                         \n",
    "    return [vocabulary[i] for i in sorted_tdif][-n:], [vocabulary[i] for i in sorted_tdif][:n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e8cd9de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List the 10 words whose presence and absence most strongly predicts that the article\n",
    "#belongs to specific category for each five categories -unigram\n",
    "n = 10\n",
    "presencesUnigram=[]\n",
    "absencesUnigram=[]\n",
    "presencesBigram=[]\n",
    "absencesBigram=[]\n",
    "for l in label_values:\n",
    "    #presences and absences of category for unigram\n",
    "    presenceUn, absenceUn = get_N_TfidfTransformer_values(df, l, n, ngrams[\"unigram\"])\n",
    "    #presences and absences of category for bigram\n",
    "    presenceBi, absenceBi = get_N_TfidfTransformer_values(df, l, n, ngrams[\"bigram\"])\n",
    "    presencesUnigram.append(presenceUn)\n",
    "    absencesUnigram.append(absenceUn)\n",
    "    presencesBigram.append(presenceBi)\n",
    "    absencesBigram.append(absenceBi)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04053b9b",
   "metadata": {},
   "source": [
    "<h1  style=\"color:#805b36 ;background:#FFD8B2\"> List the 10 words whose presence most strongly predicts that the article belongs to specific category for each five categories</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f9b28a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>decelerate</td>\n",
       "      <td>orange</td>\n",
       "      <td>deceleration</td>\n",
       "      <td>opts</td>\n",
       "      <td>decentralise</td>\n",
       "      <td>decisiveness</td>\n",
       "      <td>decker</td>\n",
       "      <td>declaration</td>\n",
       "      <td>orchestrated</td>\n",
       "      <td>invests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>hurdlers</td>\n",
       "      <td>hurdler</td>\n",
       "      <td>hurdle</td>\n",
       "      <td>hunt</td>\n",
       "      <td>hungry</td>\n",
       "      <td>hunger</td>\n",
       "      <td>hung</td>\n",
       "      <td>hundredth</td>\n",
       "      <td>hurry</td>\n",
       "      <td>karl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politics</th>\n",
       "      <td>hutus</td>\n",
       "      <td>hutu</td>\n",
       "      <td>hussein</td>\n",
       "      <td>hurts</td>\n",
       "      <td>hurting</td>\n",
       "      <td>hurt</td>\n",
       "      <td>hurriedly</td>\n",
       "      <td>hurdles</td>\n",
       "      <td>horror</td>\n",
       "      <td>zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entertainment</th>\n",
       "      <td>merengue</td>\n",
       "      <td>chieko</td>\n",
       "      <td>merchants</td>\n",
       "      <td>merchandise</td>\n",
       "      <td>mention</td>\n",
       "      <td>mental</td>\n",
       "      <td>mendelsohn</td>\n",
       "      <td>menagerie</td>\n",
       "      <td>cheshire</td>\n",
       "      <td>zutons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tech</th>\n",
       "      <td>indicted</td>\n",
       "      <td>indicator</td>\n",
       "      <td>indicative</td>\n",
       "      <td>indicating</td>\n",
       "      <td>indexed</td>\n",
       "      <td>independently</td>\n",
       "      <td>indelible</td>\n",
       "      <td>indefinitely</td>\n",
       "      <td>incumbent</td>\n",
       "      <td>jordanian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       1          2             3            4             5   \\\n",
       "business       decelerate     orange  deceleration         opts  decentralise   \n",
       "sport            hurdlers    hurdler        hurdle         hunt        hungry   \n",
       "politics            hutus       hutu       hussein        hurts       hurting   \n",
       "entertainment    merengue     chieko     merchants  merchandise       mention   \n",
       "tech             indicted  indicator    indicative   indicating       indexed   \n",
       "\n",
       "                          6           7             8             9   \\\n",
       "business        decisiveness      decker   declaration  orchestrated   \n",
       "sport                 hunger        hung     hundredth         hurry   \n",
       "politics                hurt   hurriedly       hurdles        horror   \n",
       "entertainment         mental  mendelsohn     menagerie      cheshire   \n",
       "tech           independently   indelible  indefinitely     incumbent   \n",
       "\n",
       "                      10  \n",
       "business         invests  \n",
       "sport               karl  \n",
       "politics            zone  \n",
       "entertainment     zutons  \n",
       "tech           jordanian  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(presencesUnigram, columns=[i for i in range(1,11)], index=label_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "14e776d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>freezing tariffs</td>\n",
       "      <td>freezing temperatures</td>\n",
       "      <td>freight capacity</td>\n",
       "      <td>freight carrying</td>\n",
       "      <td>french car</td>\n",
       "      <td>french carmaker</td>\n",
       "      <td>french champagne</td>\n",
       "      <td>french designer</td>\n",
       "      <td>freeze offered</td>\n",
       "      <td>zurich and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>had match</td>\n",
       "      <td>had me</td>\n",
       "      <td>had miserable</td>\n",
       "      <td>had most</td>\n",
       "      <td>had mountain</td>\n",
       "      <td>had nine</td>\n",
       "      <td>had number</td>\n",
       "      <td>had on</td>\n",
       "      <td>had lied</td>\n",
       "      <td>zvonareva while</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politics</th>\n",
       "      <td>happened was</td>\n",
       "      <td>happening even</td>\n",
       "      <td>happening round</td>\n",
       "      <td>happening to</td>\n",
       "      <td>happens and</td>\n",
       "      <td>happens both</td>\n",
       "      <td>happens is</td>\n",
       "      <td>happens it</td>\n",
       "      <td>happened she</td>\n",
       "      <td>zone the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entertainment</th>\n",
       "      <td>girl was</td>\n",
       "      <td>girl with</td>\n",
       "      <td>girlfriend for</td>\n",
       "      <td>girlfriend motley</td>\n",
       "      <td>girlfriend or</td>\n",
       "      <td>girlfriends at</td>\n",
       "      <td>girls aloud</td>\n",
       "      <td>girls and</td>\n",
       "      <td>gill champion</td>\n",
       "      <td>zutons at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tech</th>\n",
       "      <td>homes is</td>\n",
       "      <td>homes that</td>\n",
       "      <td>homes this</td>\n",
       "      <td>homes will</td>\n",
       "      <td>homework but</td>\n",
       "      <td>honda and</td>\n",
       "      <td>honda asimo</td>\n",
       "      <td>honda have</td>\n",
       "      <td>homes for</td>\n",
       "      <td>many elections</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             1                      2                 3   \\\n",
       "business       freezing tariffs  freezing temperatures  freight capacity   \n",
       "sport                 had match                 had me     had miserable   \n",
       "politics           happened was         happening even   happening round   \n",
       "entertainment          girl was              girl with    girlfriend for   \n",
       "tech                   homes is             homes that        homes this   \n",
       "\n",
       "                              4              5                6   \\\n",
       "business        freight carrying     french car  french carmaker   \n",
       "sport                   had most   had mountain         had nine   \n",
       "politics            happening to    happens and     happens both   \n",
       "entertainment  girlfriend motley  girlfriend or   girlfriends at   \n",
       "tech                  homes will   homework but        honda and   \n",
       "\n",
       "                             7                8               9   \\\n",
       "business       french champagne  french designer  freeze offered   \n",
       "sport                had number           had on        had lied   \n",
       "politics             happens is       happens it    happened she   \n",
       "entertainment       girls aloud        girls and   gill champion   \n",
       "tech                honda asimo       honda have       homes for   \n",
       "\n",
       "                            10  \n",
       "business            zurich and  \n",
       "sport          zvonareva while  \n",
       "politics              zone the  \n",
       "entertainment        zutons at  \n",
       "tech            many elections  "
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(presencesBigram, columns=[i for i in range(1,11)], index=label_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76886ba",
   "metadata": {},
   "source": [
    "<h1  style=\"color:#805b36 ;background:#FFD8B2\"> List the 10 words whose absence most strongly predicts that the article belongs to specific category for each five categories</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "996a3837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>the</td>\n",
       "      <td>to</td>\n",
       "      <td>of</td>\n",
       "      <td>in</td>\n",
       "      <td>and</td>\n",
       "      <td>said</td>\n",
       "      <td>for</td>\n",
       "      <td>that</td>\n",
       "      <td>it</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>the</td>\n",
       "      <td>to</td>\n",
       "      <td>of</td>\n",
       "      <td>in</td>\n",
       "      <td>and</td>\n",
       "      <td>for</td>\n",
       "      <td>but</td>\n",
       "      <td>on</td>\n",
       "      <td>at</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politics</th>\n",
       "      <td>to</td>\n",
       "      <td>in</td>\n",
       "      <td>the</td>\n",
       "      <td>of</td>\n",
       "      <td>and</td>\n",
       "      <td>on</td>\n",
       "      <td>is</td>\n",
       "      <td>for</td>\n",
       "      <td>be</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entertainment</th>\n",
       "      <td>the</td>\n",
       "      <td>in</td>\n",
       "      <td>of</td>\n",
       "      <td>and</td>\n",
       "      <td>to</td>\n",
       "      <td>on</td>\n",
       "      <td>for</td>\n",
       "      <td>was</td>\n",
       "      <td>has</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tech</th>\n",
       "      <td>to</td>\n",
       "      <td>the</td>\n",
       "      <td>and</td>\n",
       "      <td>of</td>\n",
       "      <td>in</td>\n",
       "      <td>that</td>\n",
       "      <td>it</td>\n",
       "      <td>is</td>\n",
       "      <td>for</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                1    2    3    4    5     6    7     8    9     10\n",
       "business       the   to   of   in  and  said  for  that   it    is\n",
       "sport          the   to   of   in  and   for  but    on   at  with\n",
       "politics        to   in  the   of  and    on   is   for   be  said\n",
       "entertainment  the   in   of  and   to    on  for   was  has  with\n",
       "tech            to  the  and   of   in  that   it    is  for    be"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(absencesUnigram, columns=[i for i in range(1,11)], index=label_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e3a166c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>in the</td>\n",
       "      <td>of the</td>\n",
       "      <td>to the</td>\n",
       "      <td>for the</td>\n",
       "      <td>on the</td>\n",
       "      <td>that the</td>\n",
       "      <td>said the</td>\n",
       "      <td>and the</td>\n",
       "      <td>the us</td>\n",
       "      <td>by the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>in the</td>\n",
       "      <td>of the</td>\n",
       "      <td>for the</td>\n",
       "      <td>at the</td>\n",
       "      <td>to the</td>\n",
       "      <td>on the</td>\n",
       "      <td>to be</td>\n",
       "      <td>he said</td>\n",
       "      <td>with the</td>\n",
       "      <td>year old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politics</th>\n",
       "      <td>of the</td>\n",
       "      <td>in the</td>\n",
       "      <td>to the</td>\n",
       "      <td>said the</td>\n",
       "      <td>he said</td>\n",
       "      <td>on the</td>\n",
       "      <td>to be</td>\n",
       "      <td>for the</td>\n",
       "      <td>and the</td>\n",
       "      <td>would be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entertainment</th>\n",
       "      <td>of the</td>\n",
       "      <td>in the</td>\n",
       "      <td>at the</td>\n",
       "      <td>to the</td>\n",
       "      <td>on the</td>\n",
       "      <td>for the</td>\n",
       "      <td>to be</td>\n",
       "      <td>and the</td>\n",
       "      <td>has been</td>\n",
       "      <td>with the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tech</th>\n",
       "      <td>of the</td>\n",
       "      <td>in the</td>\n",
       "      <td>to be</td>\n",
       "      <td>to the</td>\n",
       "      <td>on the</td>\n",
       "      <td>for the</td>\n",
       "      <td>that the</td>\n",
       "      <td>will be</td>\n",
       "      <td>it is</td>\n",
       "      <td>and the</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   1       2        3         4        5         6         7   \\\n",
       "business       in the  of the   to the   for the   on the  that the  said the   \n",
       "sport          in the  of the  for the    at the   to the    on the     to be   \n",
       "politics       of the  in the   to the  said the  he said    on the     to be   \n",
       "entertainment  of the  in the   at the    to the   on the   for the     to be   \n",
       "tech           of the  in the    to be    to the   on the   for the  that the   \n",
       "\n",
       "                    8         9         10  \n",
       "business       and the    the us    by the  \n",
       "sport          he said  with the  year old  \n",
       "politics       for the   and the  would be  \n",
       "entertainment  and the  has been  with the  \n",
       "tech           will be     it is   and the  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(absencesBigram, columns=[i for i in range(1,11)], index=label_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e09d47",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#694382; background: #ECCFFF\"> Analysis Of Part 3-a </h1>\n",
    "<ul>\n",
    "    <li>As we can see above, the words whose presence most strongly predicts the correct article category are related to that category. These words are not stop words. Because, in the most of the articles, stop words occur many times. So, the presence of the stop words actually does not predict the correct article category. But, these related words provide a better prediction.</li>\n",
    "     <li>However, the words whose absence most strongly predicts the correct article category are mostly stop words, such as \"to\", \"the\", \"and\", \"of\", \"in\", \"is\" etc. One of the reason for these words are stop words is that these stop words occur many times in articles. This causes misclassification. The Naive Bayes Model can mispredict some instances because these words are in all categories. So, the absence of these words prevents misclassification and the other words in articles are more related to that relevant category.</li>\n",
    "</ul>\n",
    "<h1 style=\"color:#c63737 ; background:#FFCDD2\"> Reimplementation Of Naive Bayes using TF-IDF </h1>\n",
    "Since we will use TF-IDF, we need to use TfidfTransformer and its result matrixes to train our data. So we will reimplement preprocess part of our Naive Bayes and then we can do the same process in training and predicting parts like we did in part 2. Reimplementation of Naive bayes is actually instead uf using CountVectorizer matrixes , we will do the TfidfTransformer to the resulting matrixes of CountVectorizer and use the TfidfTransformer matrixes for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "064a72de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CountVectorizer_results_using_TfidfTransformer(x_train, x_test, ngram_range, stopwords=frozenset()):\n",
    "    #count vectorizer results\n",
    "    vect = CountVectorizer(ngram_range=ngram_range, stop_words=stopwords)\n",
    "    train_cv = vect.fit_transform(x_train)\n",
    "    test_cv = vect.transform(x_test)\n",
    "    \n",
    "    tfidf_Transformer = TfidfTransformer()\n",
    "    #train and test matrixes using TfidfTransformer with CountVectorizer result matrixes\n",
    "    train_fidf_cv = tfidf_Transformer.fit_transform(train_cv.toarray())\n",
    "    vocabulary_list = tfidf_Transformer.get_feature_names_out()\n",
    "    test_fidf_cv = tfidf_Transformer.transform(test_cv.toarray())\n",
    "    \n",
    "    train_words_matrix = train_fidf_cv.toarray()\n",
    "    test_words_matrix = test_fidf_cv.toarray()\n",
    "    \n",
    "    #return train matrix ,test matrix and vocabulary list\n",
    "    return train_words_matrix, test_words_matrix, vocabulary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1ec54949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Prediction Time : 1.7061960697174072 sn\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "\n",
    "train_words_matrix, test_words_matrix, vocabulary_list = get_CountVectorizer_results_using_TfidfTransformer(X_train, X_test, \n",
    "                                                                                       ngrams[\"unigram\"])\n",
    "\n",
    "wordCountsOfLabels, total_counts_of_labels, logs_of_each_labels = naive_bayes_fit(X_train, y_train,\n",
    "                                                                                train_words_matrix, vocabulary_list,\n",
    "                                                                                label_values)\n",
    "\n",
    "y_preds_unigram_part3_a = naive_bayes_predict(test_words_matrix, wordCountsOfLabels, total_counts_of_labels, logs_of_each_labels,\n",
    "                        label_values , vocabulary_list)\n",
    "\n",
    "print(\"Training and Prediction Time : {} sn\".format((time.time()-st)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6ac7977b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Prediction Time : 6.50318169593811 sn\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "train_words_matrix, test_words_matrix, vocabulary_list = get_CountVectorizer_results_using_TfidfTransformer(X_train, X_test, \n",
    "                                                                                       ngrams[\"bigram\"])\n",
    "\n",
    "wordCountsOfLabels, total_counts_of_labels, logs_of_each_labels = naive_bayes_fit(X_train, y_train,\n",
    "                                                                                train_words_matrix, vocabulary_list,\n",
    "                                                                                label_values)\n",
    "\n",
    "y_preds_bigram_part3_a = naive_bayes_predict(test_words_matrix, wordCountsOfLabels, total_counts_of_labels, logs_of_each_labels,\n",
    "                        label_values , vocabulary_list)\n",
    "\n",
    "print(\"Training and Prediction Time : {} sn\".format((time.time()-st)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaa48c3",
   "metadata": {},
   "source": [
    "<h1 style=\" color:#c63737 ; background:#FFCDD2\"> Part 3-b: Stopwords </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e1bacdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List the 10 words whose presence and absence most strongly predicts that the article\n",
    "#belongs to specific category for each five categories. Stopwords are removed.\n",
    "n = 10\n",
    "presencesUnigramStopwords=[]\n",
    "absencesUnigramStopwords=[]\n",
    "presencesBigramStopwords=[]\n",
    "absencesBigramStopwords=[]\n",
    "for l in label_values:\n",
    "    #presences and absences of category for unigram\n",
    "    presenceUn, absenceUn = get_N_TfidfTransformer_values(df, l, n, ngrams[\"unigram\"], stopwords=ENGLISH_STOP_WORDS)\n",
    "    #presences and absences of category for bigram\n",
    "    presenceBi, absenceBi = get_N_TfidfTransformer_values(df, l, n, ngrams[\"bigram\"], stopwords=ENGLISH_STOP_WORDS)\n",
    "    presencesUnigramStopwords.append(presenceUn)\n",
    "    absencesUnigramStopwords.append(absenceUn)\n",
    "    presencesBigramStopwords.append(presenceBi)\n",
    "    absencesBigramStopwords.append(absenceBi)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94defc1b",
   "metadata": {},
   "source": [
    "<h1  style=\"color:#805b36 ;background:#FFD8B2\"> List the 10 words whose presence most strongly predicts that the article belongs to specific category for each five categories -Stopwords are Removed</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "bb9c50d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>cynthia</td>\n",
       "      <td>cyprus</td>\n",
       "      <td>offically</td>\n",
       "      <td>dai</td>\n",
       "      <td>offerings</td>\n",
       "      <td>daimerchrysler</td>\n",
       "      <td>daimlerchrylser</td>\n",
       "      <td>dainik</td>\n",
       "      <td>officers</td>\n",
       "      <td>intifada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>parling</td>\n",
       "      <td>parlour</td>\n",
       "      <td>dorival</td>\n",
       "      <td>doriva</td>\n",
       "      <td>parnaby</td>\n",
       "      <td>parr</td>\n",
       "      <td>dope</td>\n",
       "      <td>dons</td>\n",
       "      <td>pardew</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politics</th>\n",
       "      <td>humiliation</td>\n",
       "      <td>humiliating</td>\n",
       "      <td>humberside</td>\n",
       "      <td>humankind</td>\n",
       "      <td>humanitarian</td>\n",
       "      <td>hulme</td>\n",
       "      <td>hug</td>\n",
       "      <td>hrh</td>\n",
       "      <td>humour</td>\n",
       "      <td>zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entertainment</th>\n",
       "      <td>heir</td>\n",
       "      <td>heels</td>\n",
       "      <td>hecuba</td>\n",
       "      <td>hectoring</td>\n",
       "      <td>hectic</td>\n",
       "      <td>hecklers</td>\n",
       "      <td>heavyweights</td>\n",
       "      <td>heavier</td>\n",
       "      <td>hellboy</td>\n",
       "      <td>zutons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tech</th>\n",
       "      <td>einar</td>\n",
       "      <td>educations</td>\n",
       "      <td>eff</td>\n",
       "      <td>efficacy</td>\n",
       "      <td>pipex</td>\n",
       "      <td>pipes</td>\n",
       "      <td>pipeline</td>\n",
       "      <td>efficiencies</td>\n",
       "      <td>eeoc</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        1            2           3          4             5   \\\n",
       "business           cynthia       cyprus   offically        dai     offerings   \n",
       "sport              parling      parlour     dorival     doriva       parnaby   \n",
       "politics       humiliation  humiliating  humberside  humankind  humanitarian   \n",
       "entertainment         heir        heels      hecuba  hectoring        hectic   \n",
       "tech                 einar   educations         eff   efficacy         pipex   \n",
       "\n",
       "                           6                7             8         9   \\\n",
       "business       daimerchrysler  daimlerchrylser        dainik  officers   \n",
       "sport                    parr             dope          dons    pardew   \n",
       "politics                hulme              hug           hrh    humour   \n",
       "entertainment        hecklers     heavyweights       heavier   hellboy   \n",
       "tech                    pipes         pipeline  efficiencies      eeoc   \n",
       "\n",
       "                     10  \n",
       "business       intifada  \n",
       "sport                00  \n",
       "politics           zone  \n",
       "entertainment    zutons  \n",
       "tech                 00  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(presencesUnigramStopwords, columns=[i for i in range(1,11)], index=label_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "7d9f9d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>expand robustly</td>\n",
       "      <td>expanded 15</td>\n",
       "      <td>expanded 2004</td>\n",
       "      <td>expanded 25</td>\n",
       "      <td>expanded annual</td>\n",
       "      <td>expanded breakneck</td>\n",
       "      <td>expanded past</td>\n",
       "      <td>expanded service</td>\n",
       "      <td>expand production</td>\n",
       "      <td>zurich london</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>game hot</td>\n",
       "      <td>game ideal</td>\n",
       "      <td>game igor</td>\n",
       "      <td>game illusions</td>\n",
       "      <td>game impartial</td>\n",
       "      <td>game insisted</td>\n",
       "      <td>game intention</td>\n",
       "      <td>game involved</td>\n",
       "      <td>game hosts</td>\n",
       "      <td>zvonareva wimbledon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politics</th>\n",
       "      <td>force used</td>\n",
       "      <td>force world</td>\n",
       "      <td>forced 1967</td>\n",
       "      <td>forced apologise</td>\n",
       "      <td>forced bat</td>\n",
       "      <td>forced change</td>\n",
       "      <td>forced decide</td>\n",
       "      <td>forced deny</td>\n",
       "      <td>force tuesday</td>\n",
       "      <td>zone scottish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entertainment</th>\n",
       "      <td>film overall</td>\n",
       "      <td>film new</td>\n",
       "      <td>film official</td>\n",
       "      <td>film opened</td>\n",
       "      <td>film opening</td>\n",
       "      <td>film organisers</td>\n",
       "      <td>film original</td>\n",
       "      <td>film originally</td>\n",
       "      <td>film nominations</td>\n",
       "      <td>zutons 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tech</th>\n",
       "      <td>labs needed</td>\n",
       "      <td>labs set</td>\n",
       "      <td>labs shut</td>\n",
       "      <td>labs synergies</td>\n",
       "      <td>cafe opening</td>\n",
       "      <td>lack confidence</td>\n",
       "      <td>lack dvd</td>\n",
       "      <td>cafe killed</td>\n",
       "      <td>cafes 200</td>\n",
       "      <td>00 minute</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            1            2              3                 4   \\\n",
       "business       expand robustly  expanded 15  expanded 2004       expanded 25   \n",
       "sport                 game hot   game ideal      game igor    game illusions   \n",
       "politics            force used  force world    forced 1967  forced apologise   \n",
       "entertainment     film overall     film new  film official       film opened   \n",
       "tech               labs needed     labs set      labs shut    labs synergies   \n",
       "\n",
       "                            5                   6               7   \\\n",
       "business       expanded annual  expanded breakneck   expanded past   \n",
       "sport           game impartial       game insisted  game intention   \n",
       "politics            forced bat       forced change   forced decide   \n",
       "entertainment     film opening     film organisers   film original   \n",
       "tech              cafe opening     lack confidence        lack dvd   \n",
       "\n",
       "                             8                  9                    10  \n",
       "business       expanded service  expand production        zurich london  \n",
       "sport             game involved         game hosts  zvonareva wimbledon  \n",
       "politics            forced deny      force tuesday        zone scottish  \n",
       "entertainment   film originally   film nominations            zutons 20  \n",
       "tech                cafe killed          cafes 200            00 minute  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(presencesBigramStopwords, columns=[i for i in range(1,11)], index=label_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a29aa1b",
   "metadata": {},
   "source": [
    "<h1  style=\"color:#805b36 ;background:#FFD8B2\"> List the 10 words whose absence most strongly predicts that the article belongs to specific category for each five categories -Stopwords are Removed</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "551b513f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>said</td>\n",
       "      <td>year</td>\n",
       "      <td>new</td>\n",
       "      <td>market</td>\n",
       "      <td>company</td>\n",
       "      <td>firm</td>\n",
       "      <td>mr</td>\n",
       "      <td>chief</td>\n",
       "      <td>2004</td>\n",
       "      <td>years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>said</td>\n",
       "      <td>year</td>\n",
       "      <td>time</td>\n",
       "      <td>win</td>\n",
       "      <td>game</td>\n",
       "      <td>world</td>\n",
       "      <td>just</td>\n",
       "      <td>old</td>\n",
       "      <td>won</td>\n",
       "      <td>team</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politics</th>\n",
       "      <td>said</td>\n",
       "      <td>mr</td>\n",
       "      <td>government</td>\n",
       "      <td>people</td>\n",
       "      <td>minister</td>\n",
       "      <td>labour</td>\n",
       "      <td>election</td>\n",
       "      <td>new</td>\n",
       "      <td>party</td>\n",
       "      <td>told</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entertainment</th>\n",
       "      <td>said</td>\n",
       "      <td>year</td>\n",
       "      <td>film</td>\n",
       "      <td>new</td>\n",
       "      <td>best</td>\n",
       "      <td>star</td>\n",
       "      <td>including</td>\n",
       "      <td>time</td>\n",
       "      <td>uk</td>\n",
       "      <td>years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tech</th>\n",
       "      <td>said</td>\n",
       "      <td>people</td>\n",
       "      <td>new</td>\n",
       "      <td>use</td>\n",
       "      <td>year</td>\n",
       "      <td>technology</td>\n",
       "      <td>make</td>\n",
       "      <td>way</td>\n",
       "      <td>mr</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 1       2           3       4         5           6   \\\n",
       "business       said    year         new  market   company        firm   \n",
       "sport          said    year        time     win      game       world   \n",
       "politics       said      mr  government  people  minister      labour   \n",
       "entertainment  said    year        film     new      best        star   \n",
       "tech           said  people         new     use      year  technology   \n",
       "\n",
       "                      7      8      9      10  \n",
       "business              mr  chief   2004  years  \n",
       "sport               just    old    won   team  \n",
       "politics        election    new  party   told  \n",
       "entertainment  including   time     uk  years  \n",
       "tech                make    way     mr   like  "
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(absencesUnigramStopwords, columns=[i for i in range(1,11)], index=label_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "4c9e16f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>chief executive</td>\n",
       "      <td>analysts said</td>\n",
       "      <td>economic growth</td>\n",
       "      <td>stock market</td>\n",
       "      <td>new york</td>\n",
       "      <td>oil prices</td>\n",
       "      <td>said mr</td>\n",
       "      <td>said statement</td>\n",
       "      <td>stock exchange</td>\n",
       "      <td>news agency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>year old</td>\n",
       "      <td>told bbc</td>\n",
       "      <td>grand slam</td>\n",
       "      <td>bbc sport</td>\n",
       "      <td>second half</td>\n",
       "      <td>world cup</td>\n",
       "      <td>australian open</td>\n",
       "      <td>manchester united</td>\n",
       "      <td>champions league</td>\n",
       "      <td>fly half</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politics</th>\n",
       "      <td>tony blair</td>\n",
       "      <td>prime minister</td>\n",
       "      <td>general election</td>\n",
       "      <td>mr blair</td>\n",
       "      <td>told bbc</td>\n",
       "      <td>michael howard</td>\n",
       "      <td>said mr</td>\n",
       "      <td>liberal democrats</td>\n",
       "      <td>gordon brown</td>\n",
       "      <td>lib dems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entertainment</th>\n",
       "      <td>year old</td>\n",
       "      <td>new york</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>box office</td>\n",
       "      <td>best film</td>\n",
       "      <td>won best</td>\n",
       "      <td>best director</td>\n",
       "      <td>best actress</td>\n",
       "      <td>named best</td>\n",
       "      <td>film festival</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tech</th>\n",
       "      <td>said mr</td>\n",
       "      <td>bbc news</td>\n",
       "      <td>told bbc</td>\n",
       "      <td>news website</td>\n",
       "      <td>mobile phone</td>\n",
       "      <td>mobile phones</td>\n",
       "      <td>let people</td>\n",
       "      <td>high speed</td>\n",
       "      <td>digital music</td>\n",
       "      <td>consumer electronics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            1               2                 3   \\\n",
       "business       chief executive   analysts said   economic growth   \n",
       "sport                 year old        told bbc        grand slam   \n",
       "politics            tony blair  prime minister  general election   \n",
       "entertainment         year old        new york       los angeles   \n",
       "tech                   said mr        bbc news          told bbc   \n",
       "\n",
       "                         4             5               6                7   \\\n",
       "business       stock market      new york      oil prices          said mr   \n",
       "sport             bbc sport   second half       world cup  australian open   \n",
       "politics           mr blair      told bbc  michael howard          said mr   \n",
       "entertainment    box office     best film        won best    best director   \n",
       "tech           news website  mobile phone   mobile phones       let people   \n",
       "\n",
       "                              8                 9                     10  \n",
       "business          said statement    stock exchange           news agency  \n",
       "sport          manchester united  champions league              fly half  \n",
       "politics       liberal democrats      gordon brown              lib dems  \n",
       "entertainment       best actress        named best         film festival  \n",
       "tech                  high speed     digital music  consumer electronics  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(absencesBigramStopwords, columns=[i for i in range(1,11)], index=label_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896c2d52",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#694382; background: #ECCFFF\"> Analysis Of Part 3-b </h1>\n",
    "<ul>\n",
    "    <li>In this part, we remove the stop words from dictionary and then run the naive bayes algorithm. When we take a look above, we can easily say that most of the non-stop words whose presence most strongly predicts the correct article category are relevant to predicted category. Also, there are not many of these words in other categories. Therefore, these relevant non-stop words provide a better prediction and increase the accuracy of the model.</li>\n",
    "     <li>The words whose absence most strongly predicts the correct article will no longer be stop words because stop words are removed. Stop words are insignificant for classifciation. So, absence of these non-stop words provides a better prediction.</li>\n",
    "</ul>\n",
    "\n",
    "<h1 style=\"color:#c63737 ; background:#FFCDD2\"> Running Naive Bayes with English Stop Words Removed</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "745627ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Prediction Time : 0.999222993850708 sn\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "train_words_matrix, test_words_matrix, vocabulary_list = get_CountVectorizer_results(X_train, X_test, \n",
    "                                                                                       ngrams[\"unigram\"],\n",
    "                                                                                  stopwords=ENGLISH_STOP_WORDS)\n",
    "\n",
    "wordCountsOfLabels, total_counts_of_labels, logs_of_each_labels = naive_bayes_fit(X_train, y_train,\n",
    "                                                                                train_words_matrix, vocabulary_list,\n",
    "                                                                                label_values)\n",
    "\n",
    "y_preds_unigram_part3_b_stopwords = naive_bayes_predict(test_words_matrix, wordCountsOfLabels, total_counts_of_labels, logs_of_each_labels,\n",
    "                        label_values , vocabulary_list)\n",
    "\n",
    "print(\"Training and Prediction Time : {} sn\".format((time.time()-st)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "480c8337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Prediction Time : 1.742386817932129 sn\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "train_words_matrix, test_words_matrix, vocabulary_list = get_CountVectorizer_results(X_train, X_test, \n",
    "                                                                                       ngrams[\"bigram\"],\n",
    "                                                                                  stopwords=ENGLISH_STOP_WORDS)\n",
    "\n",
    "wordCountsOfLabels, total_counts_of_labels, logs_of_each_labels = naive_bayes_fit(X_train, y_train,\n",
    "                                                                                train_words_matrix, vocabulary_list,\n",
    "                                                                                label_values)\n",
    "\n",
    "y_preds_bigram_part3_b_stopwords = naive_bayes_predict(test_words_matrix, wordCountsOfLabels, total_counts_of_labels, logs_of_each_labels,\n",
    "                        label_values , vocabulary_list)\n",
    "\n",
    "print(\"Training and Prediction Time : {} sn\".format((time.time()-st)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4855e48",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px; font-weight: 600;text-transform:uppercase; color:#256029; background:#C8E6C9\"> Part 4:   Calculation of Accuracy </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "63b94840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 2 Results : \n",
      "Unigram Accuracy - TF-IDF is not used :  0.9194630872483222\n",
      "Bigram Accuracy - TF-IDF is not used :  0.930648769574944\n"
     ]
    }
   ],
   "source": [
    "print(\"Part 2 Results : \")\n",
    "print(\"Unigram Accuracy - TF-IDF is not used : \", accuracy_score(y_test, y_preds_unigram_part2))\n",
    "print(\"Bigram Accuracy - TF-IDF is not used : \", accuracy_score(y_test, y_preds_bigram_part2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3767b009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 3-a Results : \n",
      "Unigram Accuracy - TF-IDF used : 0.9172259507829977\n",
      "Bigram Accuracy - TF-IDF used :  0.9284116331096197\n"
     ]
    }
   ],
   "source": [
    "print(\"Part 3-a Results : \")\n",
    "print(\"Unigram Accuracy - TF-IDF used :\", accuracy_score(y_test,y_preds_unigram_part3_a))\n",
    "print(\"Bigram Accuracy - TF-IDF used : \", accuracy_score(y_test,y_preds_bigram_part3_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ae88efb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 3-b Results : \n",
      "Unigram Accuracy - Stopwords are removed:  0.959731543624161\n",
      "Bigram Accuracy - Stopwords are removed :  0.9686800894854586\n"
     ]
    }
   ],
   "source": [
    "print(\"Part 3-b Results : \")\n",
    "print(\"Unigram Accuracy - Stopwords are removed: \", accuracy_score(y_test,y_preds_unigram_part3_b_stopwords))\n",
    "print(\"Bigram Accuracy - Stopwords are removed : \", accuracy_score(y_test,y_preds_bigram_part3_b_stopwords))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c9f49d",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#694382; background: #ECCFFF\"> Analysis Of Part 4 </h1>\n",
    "<ul>\n",
    "    <li>When we look at the results, the performance of the model is very good. If we do not use TF-IDF algorithm and not remove stopwords, accuracy of the model will be above 90%. Also, accuracy of the bigram BoW model is higher than accuracy of the unigram BoW model. One reason for this difference is that there are many phrases in the English language. A word can be related to the previous or the next word.\n",
    "    </li>\n",
    "    <li>\n",
    "        When we remove stop words, the performance of the model increases. The accuracy of the model is higher than 95%. The presence of the stop words cause misclasification. Because, stop words such as \"the\", \"in\", \"is\", \"of\" etc. are in all of the articles many times. When we test a new article which also has many stop words, the probability of these stop words is higher than most of the other words. So, this makes the classification difficult. However, if we remove stop words from the dictionary, this problem will no longer be a problem.\n",
    "    </li>   \n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
